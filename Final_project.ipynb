{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "M_S5UqpHfWov",
        "8m3m-jGgz1t7",
        "vs1GPtv15r2l",
        "3bQadTwp8DHD",
        "A-fqTDkyJMYM",
        "EBsCmYoe2_7j",
        "iDNahdZVf9TU",
        "kzbUAxrq5Y7z",
        "HqEeM5vC2Vyu",
        "Ranmu46ShtIx",
        "Af34lg8wisW2",
        "4dBWuO9lix-H",
        "pCn6WZsRi4O2",
        "itlhrajYjcWG",
        "RXZbqTgqi84n",
        "Iyo3COzpi_11",
        "XtOXi58GjS25",
        "UT2h1O6sglwA",
        "dVc5RjU_oyNP",
        "Db6gW_HrkJvX",
        "8mPlH080hIUQ",
        "20yq4Jswg2M7"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clip Model\n",
        "\n"
      ],
      "metadata": {
        "id": "zi7HKyK0gJSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clip with lighting module"
      ],
      "metadata": {
        "id": "M_S5UqpHfWov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Packages Installed and Imports"
      ],
      "metadata": {
        "id": "pVjfZQL_fpQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### installations"
      ],
      "metadata": {
        "id": "3v17GL1dzycY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install cairosvg\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install transformers\n",
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "id": "5_LpMGjgftqD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import modules"
      ],
      "metadata": {
        "id": "8m3m-jGgz1t7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import random\n",
        "from zipfile import ZipFile\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from transformers import CLIPProcessor, CLIPModel, AdamW\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import Subset, random_split\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "import json\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from transformers.models.clip.modeling_clip import CLIPOutput\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import io\n",
        "import cairosvg"
      ],
      "metadata": {
        "id": "fRm_orGzf0Hs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataSet"
      ],
      "metadata": {
        "id": "vs1GPtv15r2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset AWS"
      ],
      "metadata": {
        "id": "3bQadTwp8DHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClipSWDataset(data.Dataset):\n",
        "    def __init__(self, dataset,processor):\n",
        "        self.processor = processor\n",
        "        self.dataset=[]\n",
        "        self.filter_data(dataset)\n",
        "\n",
        "    def create_new_datum(self,datum,term,correct_image):\n",
        "        new_datum = datum.copy()\n",
        "        if self.processor(term,return_tensors=\"pt\").input_ids.to(device).shape[1] < 77:\n",
        "           new_datum['terms'] = [\"\\n \",term]\n",
        "           new_datum['sign_writing_images'] = [correct_image]\n",
        "        \n",
        "        return new_datum\n",
        "    \n",
        "    def filter_datum(self,datum):\n",
        "        return len(datum['terms']) > 1 and len(datum['sign_writing_images']) > 0 and datum['assumed_spoken_language_code'] == 'en'\n",
        "            \n",
        "    def filter_data(self,dataset):\n",
        "        self.dataset = []\n",
        "\n",
        "        for datum in dataset:\n",
        "            if self.filter_datum(datum):\n",
        "                # Got the image where there is just one image that describe the sign writing\n",
        "                correct_image = None\n",
        "                for images in datum['sign_writing_images']:\n",
        "                    if len(images) == 1:\n",
        "                        correct_image = images\n",
        "                        break\n",
        "                # Check it there is one image that describe the text\n",
        "                if correct_image:\n",
        "                    for term in datum['terms'][1:]:\n",
        "                        if len(term) < 77:\n",
        "                            new_datum = self.create_new_datum(datum,term,correct_image)\n",
        "                            self.dataset.append(new_datum)\n",
        "                        \n",
        "    def __len__(self): \n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Change it , use processor\n",
        "        preprocess_transforms = transforms.Compose([\n",
        "            transforms.Lambda(lambda rgba_img: rgba_img.convert('RGB')),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor()])\n",
        "        svg_file_path = self.dataset[index]['sign_writing_images'][0][0]\n",
        "        png_bytes = cairosvg.svg2png(url=svg_file_path)\n",
        "        curr_img = Image.open(io.BytesIO(png_bytes))  \n",
        "        image_tensor = preprocess_transforms(curr_img)\n",
        "        text = self.dataset[index][\"terms\"][1]\n",
        "        return image_tensor, text"
      ],
      "metadata": {
        "id": "MklkOlCp8nCu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split data"
      ],
      "metadata": {
        "id": "A-fqTDkyJMYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_into_train_and_test():\n",
        "    json_folder_path = 'signwriting-images/dataset-outputs-new'\n",
        "    json_files = []\n",
        "    for dirpath, _, filenames in os.walk(json_folder_path):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith('.json'):\n",
        "              json_file_path = os.path.join(dirpath, filename)\n",
        "              # Open the JSON file for reading\n",
        "              with open(json_file_path, 'r') as f:\n",
        "                  json_str = f.read()\n",
        "                  # Parse the JSON string into a Python object\n",
        "                  data = json.loads(json_str)\n",
        "              json_files.append(data)\n",
        "    \n",
        "    train_size = int(0.8 * len(json_files))\n",
        "    test_size = len(json_files) - train_size\n",
        "    train_dataset, test_dataset = random_split(json_files, [train_size, test_size])\n",
        "\n",
        "    return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "qNurG5ixIoQS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss"
      ],
      "metadata": {
        "id": "EBsCmYoe2_7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the contrastive loss function\n",
        "def contrastive_loss(image_rep, text_rep):\n",
        "    bs = image_rep.size(0)\n",
        "    device = image_rep.device\n",
        "    similarity_matrix = torch.matmul(image_rep, text_rep.T)\n",
        "    mask = torch.eye(bs, device=device).bool()\n",
        "    sim_pos = torch.diagonal(similarity_matrix)\n",
        "    sim_neg = similarity_matrix[~mask].view(bs, -1).max(dim=1).values\n",
        "    loss = (-torch.log(sim_pos / (sim_pos + sim_neg))).mean()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "aF_nISG02415"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "1kKBf2pgfisl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPTrainer(pl.LightningModule):\n",
        "    def __init__(self, model_name_or_path, loss_fn,learning_rate=3e-5,batch_size=64):\n",
        "        super().__init__()\n",
        "        self.processor  = CLIPProcessor.from_pretrained(model_name_or_path)\n",
        "        self.model: CLIPModel = CLIPModel.from_pretrained(model_name_or_path)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.loss_fn = loss_fn\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask, image):\n",
        "        return self.model(input_ids=input_ids, attention_mask=attention_mask, visual_inputs=image)\n",
        "\n",
        "    def pad_text_tokenized(self, texts):\n",
        "        texts = [self.processor(text, return_tensors=\"pt\").input_ids.to(device) for text in texts]\n",
        "\n",
        "        # Pad tensors with zeros to make them the same size\n",
        "        padded_tensors = []\n",
        "        max_len = max([t.shape[1] for t in texts])\n",
        "        for t in texts:\n",
        "            padded_tensor = F.pad(t, (0, max_len - t.shape[1]), mode='constant', value=0)\n",
        "            padded_tensors.append(padded_tensor)\n",
        "\n",
        "        stacked_tensor = torch.stack(padded_tensors)\n",
        "        stacked_tensor = torch.squeeze(stacked_tensor, dim=1)\n",
        "        return stacked_tensor\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        image, texts = batch\n",
        "        stacked_tensor = self.pad_text_tokenized(texts)      \n",
        "\n",
        "        outputs: CLIPOutput = self.model(stacked_tensor, image)\n",
        "        logits_per_image, logits_per_text = outputs.logits_per_image, outputs.logits_per_text\n",
        "        loss = self.loss_fn(logits_per_image, logits_per_text)\n",
        "        self.log('train_loss', loss, on_epoch=True, logger=True, prog_bar=True,batch_size=self.batch_size)\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        image, texts = batch\n",
        "        stacked_tensor = self.pad_text_tokenized(texts)      \n",
        "\n",
        "        outputs: CLIPOutput = self.model(stacked_tensor, image)\n",
        "        logits_per_image, logits_per_text = outputs.logits_per_image, outputs.logits_per_text\n",
        "        loss = self.loss_fn(logits_per_image, logits_per_text)\n",
        "        self.log('val_loss', loss, on_epoch=True, logger=True, prog_bar=True, batch_size=self.batch_size)\n",
        "        return loss\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n",
        "    \n",
        "    def prepare_data(self):\n",
        "      train_dataset, test_dataset = split_into_train_and_test()\n",
        "      print(f\"Len Before train: {len(train_dataset)}\")\n",
        "      print(f\"Len Before test: {len(test_dataset)}\")\n",
        "      self.train_dataset = ClipSWDataset(train_dataset, self.processor)\n",
        "      self.test_dataset = ClipSWDataset(test_dataset, self.processor)\n",
        "      print(f\"Len After train: {len(self.train_dataset)}\")\n",
        "      print(f\"Len After test: {len(self.test_dataset)}\")\n",
        "\n",
        "        \n",
        "    def train_dataloader(self):\n",
        "       return DataLoader(dataset=self.train_dataset,\n",
        "                         batch_size=self.batch_size,\n",
        "                         shuffle=True,\n",
        "                         num_workers=2)\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "      return DataLoader(dataset=self.test_dataset,\n",
        "                        batch_size=self.batch_size,\n",
        "                        shuffle=False,\n",
        "                        num_workers=2)"
      ],
      "metadata": {
        "id": "iWOSUtIVfgw8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "iDNahdZVf9TU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRYgp38dvzCz"
      },
      "source": [
        "### Got zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kxHHA4UmIxHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e638305-6546-4da9-d4e0-8e8a85880064"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "SIGN_BANK_DATASET = \"signwriting-images\"\n",
        "\n",
        "!pip install -U --no-cache-dir gdown --pre\n",
        "!gdown --id 1-41JbcJCtx8tdmnGq-krrMe-6aiqVSPo # signbank\n",
        "\n",
        "with ZipFile(f'{SIGN_BANK_DATASET}.zip', 'r') as zObject:\n",
        "  zObject.extractall(path=SIGN_BANK_DATASET)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "6QDXi6FVDC0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script echo skipping\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "checkpoint_path = \"drive/MyDrive/sign_writing_trainer/\"\n",
        "checkpoint_callback = ModelCheckpoint(dirpath=checkpoint_path, filename=\"sw-model\",save_top_k=1, monitor='val_loss')\n",
        "callbacks = [checkpoint_callback]\n",
        "model = CLIPTrainer('openai/clip-vit-base-patch32',contrastive_loss, learning_rate=1e-4,batch_size=100)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "trainer = pl.Trainer(accelerator='gpu', max_epochs=10,callbacks=callbacks)\n",
        "trainer.fit(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HncaSmJAAVwY",
        "outputId": "db762ebc-84fe-4591-cedc-f8a5a7809137"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model & Create vectors"
      ],
      "metadata": {
        "id": "OudIZivyx7Bo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzbUAxrq5Y7z"
      },
      "source": [
        "### Got zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NTfk_GxV5Y8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b76fc25d-f7be-4788-fecf-eccad6ee06ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "SIGN_2_MINT_SVGS_DATASET = \"sign2mint-Svgs\"\n",
        "\n",
        "!pip install -U --no-cache-dir gdown --pre\n",
        "!gdown --id 1ZXDsSKodxbHGLTUWQRjBTc27KcUmnoYs # model\n",
        "!gdown --id 1vmakUlaApbEtVz6rq9M-1pN55aGPDfHc # sign2mint jsonl\n",
        "!gdown --id 1kryyoTYEqMRTdUCuODU1rd1fBQo7C4mY # sign2mint svgs\n",
        "\n",
        "with ZipFile(f'{SIGN_2_MINT_SVGS_DATASET}.zip', 'r') as zObject:\n",
        "  zObject.extractall(path=SIGN_2_MINT_SVGS_DATASET)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sign writing loaded model"
      ],
      "metadata": {
        "id": "HqEeM5vC2Vyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SignWritingModel:\n",
        "    def __init__(self, model_name_or_path: str, checkpoint_path: str):\n",
        "        args = dict(\n",
        "            model_name_or_path=model_name_or_path,\n",
        "            loss_fn=contrastive_loss\n",
        "        )\n",
        "        self.trainer: CLIPTrainer = CLIPTrainer.load_from_checkpoint(\n",
        "            checkpoint_path=checkpoint_path, **args)\n",
        "        self.similarity_model: CLIPModel = self.trainer.model.to(device)\n",
        "        self.processor = self.trainer.processor\n",
        "\n",
        "    def sign_writing_signature(self, text: str, image) -> torch.Tensor:\n",
        "        inputs = self.trainer.processor(text=text,images=image,return_tensors=\"pt\").to(device)\n",
        "        encoded_text = self.similarity_model.get_text_features(input_ids=inputs.input_ids)\n",
        "        encoded_image = self.similarity_model.get_image_features(pixel_values=inputs.pixel_values)\n",
        "        encoded_vector = torch.cat((encoded_image, encoded_text), dim=-1)\n",
        "        return encoded_vector"
      ],
      "metadata": {
        "id": "FyoTuM173sbK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create sign2mint vectors"
      ],
      "metadata": {
        "id": "JdFbrTALyZVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_sign2mint_vectors(model: SignWritingModel, dir_images_path: str):\n",
        "    with open('sign2mint.jsonl', 'r') as f:\n",
        "        with open(\"sign2mint-Vectors.jsonl\", \"w\") as s:\n",
        "            sign2mint = [json.loads(s) for s in list(f)]\n",
        "            # vectors_list = []\n",
        "            for row in tqdm(sign2mint):\n",
        "                uid = row[\"doc\"][\"uid\"]\n",
        "                word = row[\"captions\"][0][\"transcription\"]\n",
        "                png_data = cairosvg.svg2png(url=f'{dir_images_path}/{uid}.svg', write_to=None)\n",
        "                curr_img = Image.open(io.BytesIO(png_data))\n",
        "\n",
        "                embedding_vector = model.sign_writing_signature(\n",
        "                    text=word, image=curr_img)\n",
        "                result = {\"word\": word, \"uid\": uid, \"embedding_vector\": embedding_vector.tolist()}\n",
        "                json_string = json.dumps(result)\n",
        "                s.write(json_string + \"\\n\")"
      ],
      "metadata": {
        "id": "tH8Bub-gyABo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script echo skipping\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SignWritingModel('openai/clip-vit-base-patch32', 'sw_model.ckpt')\n",
        "store_sign2mint_vectors(model=model, dir_images_path=SIGN_2_MINT_SVGS_DATASET)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq9xmSia2oME",
        "outputId": "7d608165-d45b-4a92-efd8-7e7ad32c3871"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spoken to text"
      ],
      "metadata": {
        "id": "Ranmu46ShtIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### installations"
      ],
      "metadata": {
        "id": "I6G1podNsrRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install SpeechRecognition"
      ],
      "metadata": {
        "id": "X8eFOQybsrRW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import modules"
      ],
      "metadata": {
        "id": "YJCe2WhliKI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr"
      ],
      "metadata": {
        "id": "8FBrrtIQiK6n"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate text from spoken"
      ],
      "metadata": {
        "id": "YWd42OTJiOan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioRecorder:\n",
        "    def __init__(self):\n",
        "        self.recognizer = sr.Recognizer()\n",
        "\n",
        "    def convert_to_text(self, wav_filename: str):\n",
        "        with sr.AudioFile(wav_filename) as source:\n",
        "            audio_text = self.recognizer.record(source)\n",
        "\n",
        "        try:\n",
        "            s = self.recognizer.recognize_google(audio_text)\n",
        "            print(\"Path: \", wav_filename, \" Text: \" + s)\n",
        "        except Exception as e:\n",
        "            s = \"wave_path: \" + wav_filename + \" Exception: \" + str(e)\n",
        "            print(s)\n",
        "        return s"
      ],
      "metadata": {
        "id": "kjRYjtJ3iR8A"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate video from text"
      ],
      "metadata": {
        "id": "Af34lg8wisW2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4nwTnJDlFYy"
      },
      "source": [
        "### Got zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvlVetkalFZG",
        "outputId": "8f758183-ce3e-4f5d-bf96-7d265ff00b3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.6.6\n",
            "    Uninstalling gdown-4.6.6:\n",
            "      Successfully uninstalled gdown-4.6.6\n",
            "Successfully installed gdown-4.7.1\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Tdry97vVyXUL-aFdQ97QDElw_lb4smKL\n",
            "To: /content/font_db.zip\n",
            "100% 15.2M/15.2M [00:00<00:00, 24.2MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (uriginal): https://drive.google.com/uc?id=1_efH5P-lfg6coxkOmupOoutGVzaKgRPr\n",
            "From (redirected): https://drive.google.com/uc?id=1_efH5P-lfg6coxkOmupOoutGVzaKgRPr&confirm=t&uuid=0c97849f-e209-44d3-a940-dba816ecae10\n",
            "To: /content/sign2mint-vectors.zip\n",
            "100% 50.3M/50.3M [00:00<00:00, 51.1MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (uriginal): https://drive.google.com/uc?id=1cpmCWgpRGZeC2GMfdz2ROrrhpn6sp3Bo\n",
            "From (redirected): https://drive.google.com/uc?id=1cpmCWgpRGZeC2GMfdz2ROrrhpn6sp3Bo&confirm=t&uuid=2dc5f0ce-caeb-4823-9b71-8634d22f439e\n",
            "To: /content/sign2mint-Videos.zip\n",
            "100% 5.66G/5.66G [01:08<00:00, 82.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "FONT_DB = \"font_db\"\n",
        "SIGN_2_MINT_VECTORS_DATASET = \"sign2mint-Vectors\"\n",
        "SIGN_2_MINT_VIDEOS_DATASET = \"sign2mint-Videos\"\n",
        "\n",
        "!pip install -U --no-cache-dir gdown --pre\n",
        "!gdown --id 1Tdry97vVyXUL-aFdQ97QDElw_lb4smKL # font_db\n",
        "!gdown --id 1_efH5P-lfg6coxkOmupOoutGVzaKgRPr # sign2mint vector\n",
        "!gdown --id 1cpmCWgpRGZeC2GMfdz2ROrrhpn6sp3Bo # sign2mint videos\n",
        "\n",
        "with ZipFile(f'{FONT_DB}.zip', 'r') as zObject:\n",
        "  zObject.extractall(path=FONT_DB)\n",
        "\n",
        "subprocess.call('npm install', cwd='font_db/font_db', shell=True)\n",
        "\n",
        "with ZipFile(f'{SIGN_2_MINT_VECTORS_DATASET}.zip', 'r') as zObject:\n",
        "  zObject.extractall(path=SIGN_2_MINT_VECTORS_DATASET.split('-')[1])\n",
        "\n",
        "with ZipFile(f'{SIGN_2_MINT_VIDEOS_DATASET}.zip', 'r') as zObject:\n",
        "  zObject.extractall(path=SIGN_2_MINT_VIDEOS_DATASET.split('-')[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import modules"
      ],
      "metadata": {
        "id": "4dBWuO9lix-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import shutil\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "import torch\n",
        "import subprocess\n",
        "import cairosvg\n",
        "import io\n",
        "import numpy as np\n",
        "from moviepy.video.compositing.concatenate import concatenate_videoclips\n",
        "from moviepy.video.io.VideoFileClip import VideoFileClip"
      ],
      "metadata": {
        "id": "HfkuWBWQiyoP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "pCn6WZsRi4O2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sign writing util"
      ],
      "metadata": {
        "id": "itlhrajYjcWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def api_call_spoken2sign(payload):\n",
        "    url = 'https://pub.cl.uzh.ch/demo/signwriting/spoken2sign'\n",
        "    response = requests.post(url, json=payload).json()\n",
        "    return response['translations'][0]"
      ],
      "metadata": {
        "id": "BAPOA1rSjhBP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video util"
      ],
      "metadata": {
        "id": "RXZbqTgqi84n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concate_two_videos(first_video_name: str,\n",
        "                       second_video_name: str,\n",
        "                       final_video_name: str):\n",
        "    # Load the two video files\n",
        "    first_video = VideoFileClip(f'{first_video_name}.mp4')\n",
        "    second_video = VideoFileClip(f'{second_video_name}.mp4')\n",
        "\n",
        "    # Concatenate the clips\n",
        "    final_video = concatenate_videoclips([first_video, second_video])\n",
        "\n",
        "    # Write the final clip to a new file\n",
        "    final_video.write_videofile(f'{final_video_name}.mp4')"
      ],
      "metadata": {
        "id": "K9P-T0w6i6mm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity util"
      ],
      "metadata": {
        "id": "Iyo3COzpi_11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def diff(a, b, similar=False):\n",
        "    sim = \"\"\n",
        "    if similar is not True:\n",
        "        sim = \"Not\"\n",
        "    dot_product = np.dot(a, b)\n",
        "\n",
        "    # Calculate the magnitudes\n",
        "    magnitude_a = np.linalg.norm(a)\n",
        "    magnitude_b = np.linalg.norm(b)\n",
        "\n",
        "    # Calculate the cosine similarity\n",
        "    cosine_similarity = dot_product / (magnitude_a * magnitude_b)\n",
        "\n",
        "    return cosine_similarity\n",
        "\n",
        "\n",
        "def check_cosin(traget_vector: Tensor, data) -> list:\n",
        "    differences = []\n",
        "    for dt in data:\n",
        "        # dt = { \"text\", \"image_text_encoded\", \"pose_url\"}\n",
        "        vec = np.array(dt['embedding_vector']).flatten()\n",
        "        img = np.array(traget_vector.tolist()).flatten()\n",
        "        cos_diff = diff(vec, img)\n",
        "        differences.append((dt['word'], dt['uid'], cos_diff))\n",
        "\n",
        "    differences = sorted(differences, key=lambda diff_images: diff_images[2],reverse=True)\n",
        "\n",
        "    return differences"
      ],
      "metadata": {
        "id": "88mvchjmjFL_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get video"
      ],
      "metadata": {
        "id": "XtOXi58GjS25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FlowManager:\n",
        "    def __init__(self, model: SignWritingModel,\n",
        "                 encoded_vectors_path: str):\n",
        "        self.model = model\n",
        "\n",
        "        with open(encoded_vectors_path,'r') as f:\n",
        "            self.data = [json.loads(s) for s in list(f)]\n",
        "\n",
        "    def generate_image(self, spoken_word: str) -> Image:\n",
        "        encoded_sw = api_call_spoken2sign({\n",
        "            \"country_code\": 'us',\n",
        "            \"language_code\": 'en',\n",
        "            \"text\": spoken_word,\n",
        "            \"translation_type\": \"sent\"\n",
        "        })\n",
        "        output = subprocess.check_output(\n",
        "            f'node fsw/fsw-sign-svg {encoded_sw}', cwd='font_db/font_db', shell=True)\n",
        "        output_str = output.decode('utf-8')\n",
        "        png_bytes = cairosvg.svg2png(output_str)\n",
        "        curr_img = Image.open(io.BytesIO(png_bytes))\n",
        "\n",
        "        return curr_img\n",
        "\n",
        "    def get_word_uid(self, word: str, data):\n",
        "        for dt in data:\n",
        "            if dt['word'] == word:\n",
        "                return dt[\"uid\"]\n",
        "\n",
        "        return None\n",
        "\n",
        "    def run(self, sentence: str):\n",
        "        words_sentence: list[str] = sentence.replace('`', '').split(' ')\n",
        "\n",
        "        uid_sentence = []\n",
        "        for text in words_sentence:\n",
        "            word_uid = self.get_word_uid(text, self.data)\n",
        "            if not word_uid:\n",
        "                image: Image = self.generate_image(spoken_word=text)\n",
        "                encoded_vector: torch.Tensor = self.model.sign_writing_signature(\n",
        "                    text=text, image=image)\n",
        "                similar_tensor_images = check_cosin(traget_vector=encoded_vector,\n",
        "                                                    data=self.data)\n",
        "                word_uid = similar_tensor_images[0][1]\n",
        "\n",
        "            uid_sentence.append(f'{SIGN_2_MINT_VIDEOS_DATASET}/{word_uid}.mp4')\n",
        "\n",
        "        final_video_name = 'final_video.mp4'\n",
        "        video_files_name: list = uid_sentence\n",
        "        if len(video_files_name) > 0:\n",
        "            shutil.copy(video_files_name[0], final_video_name)\n",
        "        for video_file_name in video_files_name[1:]:\n",
        "            concate_two_videos(\n",
        "                final_video_name, video_file_name, final_video_name)"
      ],
      "metadata": {
        "id": "aM1ub8oDjWa_"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Telegram Bot"
      ],
      "metadata": {
        "id": "UT2h1O6sglwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### installations"
      ],
      "metadata": {
        "id": "dVc5RjU_oyNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install ffmpeg\n",
        "!pip install fastapi\n",
        "!pip install uvicorn\n",
        "!pip install python-telegram-bot\n",
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA8b7fzPoyNh",
        "outputId": "b0748b61-0b70-449b-c792-7941fc09c126"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.95.1-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.7)\n",
            "Collecting starlette<0.27.0,>=0.26.1 (from fastapi)\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi) (4.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.27.0,>=0.26.1->fastapi) (3.6.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.27.0,>=0.26.1->fastapi) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.27.0,>=0.26.1->fastapi) (1.3.0)\n",
            "Installing collected packages: starlette, fastapi\n",
            "Successfully installed fastapi-0.95.1 starlette-0.26.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.3)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, uvicorn\n",
            "Successfully installed h11-0.14.0 uvicorn-0.22.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-telegram-bot\n",
            "  Downloading python_telegram_bot-20.3-py3-none-any.whl (545 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m545.4/545.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx~=0.24.0 (from python-telegram-bot)\n",
            "  Downloading httpx-0.24.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx~=0.24.0->python-telegram-bot) (2022.12.7)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx~=0.24.0->python-telegram-bot)\n",
            "  Downloading httpcore-0.17.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx~=0.24.0->python-telegram-bot) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx~=0.24.0->python-telegram-bot) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx~=0.24.0->python-telegram-bot) (0.14.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx~=0.24.0->python-telegram-bot) (3.6.2)\n",
            "Installing collected packages: httpcore, httpx, python-telegram-bot\n",
            "Successfully installed httpcore-0.17.0 httpx-0.24.0 python-telegram-bot-20.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db6gW_HrkJvX"
      },
      "source": [
        "### Got files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTotLhikkJvl",
        "outputId": "df93011a-8f6a-49f7-d6e0-2fabf7ce21ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (uriginal): https://drive.google.com/uc?id=1ZXDsSKodxbHGLTUWQRjBTc27KcUmnoYs\n",
            "From (redirected): https://drive.google.com/uc?id=1ZXDsSKodxbHGLTUWQRjBTc27KcUmnoYs&confirm=t&uuid=6a5164c4-07d0-487b-b42d-45ce58c4444e\n",
            "To: /content/sw_model.ckpt\n",
            "100% 1.82G/1.82G [00:14<00:00, 122MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vmakUlaApbEtVz6rq9M-1pN55aGPDfHc\n",
            "To: /content/sign2mint.jsonl\n",
            "100% 5.30M/5.30M [00:00<00:00, 190MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install -U --no-cache-dir gdown --pre\n",
        "!gdown --id 1ZXDsSKodxbHGLTUWQRjBTc27KcUmnoYs # model\n",
        "!gdown --id 1vmakUlaApbEtVz6rq9M-1pN55aGPDfHc # sign2mint jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import modules"
      ],
      "metadata": {
        "id": "8mPlH080hIUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import subprocess\n",
        "import requests\n",
        "from telegram import Update\n",
        "from telegram.ext import Application, CommandHandler, MessageHandler, ContextTypes,filters\n",
        "from pydub import AudioSegment"
      ],
      "metadata": {
        "id": "MZ9xJI3fhKIf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global Variables"
      ],
      "metadata": {
        "id": "20yq4Jswg2M7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOKEN = '5993731273:AAHnf_LvkKJO-oWdXf_O20eDvRXKBF7Bkco'\n",
        "FFMPEG_PATH = '/usr/bin/ffmpeg'\n",
        "os.environ[\"ffmpeg\"] = FFMPEG_PATH\n",
        "\n",
        "AudioSegment.converter = FFMPEG_PATH\n",
        "AudioSegment.ffmpeg = FFMPEG_PATH\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "flow_manager = FlowManager(model=SignWritingModel(\n",
        "    model_name_or_path='openai/clip-vit-base-patch32',\n",
        "    checkpoint_path='sw_model.ckpt'),\n",
        "    encoded_vectors_path='sign2mint-vectors/sign2mint-vectors.jsonl')"
      ],
      "metadata": {
        "id": "e6AJBnoig4eB"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Commands"
      ],
      "metadata": {
        "id": "hifMsautgxMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: fix audio shit, create api for the model(easy), queues ?\n",
        "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    \"\"\"Send a message when the command /start is issued.\"\"\"\n",
        "    user = update.effective_user\n",
        "    await update.message.reply_html(\n",
        "        rf\"Hi {user.mention_html()}!\",\n",
        "    )"
      ],
      "metadata": {
        "id": "vLbRZIdWgwqQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def handle_voice_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    \"\"\"Send a message when the command /start is issued.\"\"\"\n",
        "    file_name = 'audio'\n",
        "    voice = update.message.voice\n",
        "    file = await voice.get_file()\n",
        "\n",
        "    response = requests.get(file.file_path)\n",
        "\n",
        "    with open(f'{file_name}.oga', 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    if os.path.exists(f'{file_name}.wav'):\n",
        "        os.remove(f'{file_name}.wav')\n",
        "\n",
        "    subprocess.run(['ffmpeg', '-i', f'{file_name}.oga', f'{file_name}.wav'])\n",
        "\n",
        "    recorder = AudioRecorder()\n",
        "    text = recorder.convert_to_text(wav_filename=f'{file_name}.wav')\n",
        "    await update.message.reply_text(text)\n",
        "\n",
        "    if not text.__contains__('Exception'):\n",
        "        flow_manager.run(text)\n",
        "        await context.bot.send_video(chat_id=update.effective_chat.id,\n",
        "                                     video=open('final_video.mp4', 'rb'))\n",
        "        os.remove('final_video.mp4')\n",
        "\n",
        "    os.remove(f'{file_name}.oga')\n",
        "    os.remove(f'{file_name}.wav')"
      ],
      "metadata": {
        "id": "No-5njNhhSsK"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def handle_text_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    text = update.message.text\n",
        "    flow_manager.run(text)\n",
        "    await context.bot.send_video(chat_id=update.effective_chat.id,\n",
        "                                  video=open('final_video.mp4', 'rb'))\n",
        "    os.remove('final_video.mp4')"
      ],
      "metadata": {
        "id": "DDsyQXcWhUhw"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Bot"
      ],
      "metadata": {
        "id": "fnKHV6kqhVJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "YPU47ua_4L62"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "application = Application.builder().token(TOKEN).build()\n",
        "application.add_handler(CommandHandler(\"start\", start))\n",
        "application.add_handler(MessageHandler(filters.TEXT, handle_text_message))\n",
        "application.add_handler(MessageHandler(filters.VOICE, handle_voice_message))\n",
        "\n",
        "print(\"Running application...\")\n",
        "application.run_polling()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "M-Hmbi0LhW1I",
        "outputId": "52f5b0db-4881-48f4-a713-19446e1bbde9"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running application...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-cb5bcca8c91c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running application...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mapplication\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_polling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/telegram/ext/_application.py\u001b[0m in \u001b[0;36mrun_polling\u001b[0;34m(self, poll_interval, timeout, bootstrap_retries, read_timeout, write_timeout, connect_timeout, pool_timeout, allowed_updates, drop_pending_updates, close_loop, stop_signals)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         return self.__run(\n\u001b[0m\u001b[1;32m    728\u001b[0m             updater_coroutine=self.updater.start_polling(\n\u001b[1;32m    729\u001b[0m                 \u001b[0mpoll_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpoll_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/telegram/ext/_application.py\u001b[0m in \u001b[0;36m__run\u001b[0;34m(self, updater_coroutine, stop_signals, close_loop)\u001b[0m\n\u001b[1;32m    935\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mclose_loop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m                     \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m     def create_task(\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/unix_events.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finalizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signal_handlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/selector_events.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot close a running event loop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Cannot close a running event loop"
          ]
        }
      ]
    }
  ]
}