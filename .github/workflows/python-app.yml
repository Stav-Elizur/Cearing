# This workflow will install Python dependencies, load the dataset, and run the training script on CI with cache to dataset.

name: Python application

on:
  push:

permissions:
  contents: read

env:
  DATASET_CACHE_DIR: dataset_cached

jobs:
  build:

    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python 3.10
        uses: actions/setup-python@v3
        with:
          python-version: "3.10"
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip uninstall protobuf
          pip uninstall google
          pip install protobuf==3.19.6
          touch /opt/hostedtoolcache/Python/3.10.10/x64/lib/python3.10/site-packages/google/__init__.py
          touch /opt/hostedtoolcache/Python/3.10.10/x64/lib/python3.10/site-packages/google/protobuf/internal/builder.py
          cp builder.py /opt/hostedtoolcache/Python/3.10.10/x64/lib/python3.10/site-packages/google/protobuf/internal/builder.py
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      - name: Load dataset
        run: |
          if [ ! -d "$env:DATASET_CACHE_DIR" ]; then
            mkdir "$env:DATASET_CACHE_DIR"
            python -m dataset.data
          fi

      - name: Cache dataset
        uses: actions/cache@v2
        with:
          path: ${{ env.DATASET_CACHE_DIR }}
          key: ${{ runner.os }}-dataset-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-dataset-